{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (C) 2018-2021 Intel Corporation\n",
    "# SPDX-License-Identifier:  BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Motivation: In this tutorial, we will build a Lava Process for an MNIST \n",
    "classifier, using the Lava Processes for LIF neurons and Dense connectivity. \n",
    "We will see how a single MNIST Lava Process runs on Tensorflow software \n",
    "backend. We will use the `ExecVar` mechanism to read/write neuron states \n",
    "directly._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An MNIST Classifier as a Hierarchical Process in Lava, using LIF and Dense Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This tutorial assumes that you:\n",
    "- have the Lava framework installed \n",
    "- are familiar with the *Process* concept in Lava (please refer to other Lava tutorials)\n",
    "- know the ANN to SNN conversion philosophy using rate coding or any other type of encoding\n",
    "\n",
    "### This tutorial shows \n",
    "- how a Lava Process can be run on Tensorflow backend\n",
    "- how to interact with a Lava process via `ExecVar`s, to read and write \n",
    "neuronal states\n",
    "\n",
    "### This tutorial does not\n",
    "- show how to train an MNIST classifier\n",
    "- show how to convert an ANN classifier to SNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our MNIST Classifier\n",
    "For the purposes of this tutorial we have chosen a classifier, which does not use any convolutional layers. It has a simple feed-forward architecture with ReLU activations and all-to-all dense connectivity in case of an ANN, and LIF neurons with all-to-all dense connectivity in case of an SNN, as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](mnist_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that connectivity and activation are both specified as parameters of a single layer object in an ANN implemented in Keras, whereas in a Lava Process, neuron and connection Processes are two different objects. This is indicated with a colour gradient in the ANN architecture as opposed to two distinct colours in the SNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on network training and obtaining parameters for the SNN classifier\n",
    "\n",
    "Instead of directly training the SNN classifier, we have trained the equivalent ANN in Keras using standard backpropagation. Then we have used SNNToolBox to convert the ANN to an SNN model. The model parameters of the SNN model are saved to disk.\n",
    "\n",
    "When we wish to run inference using the Lava Process, we simply load the pre-trained model parameters from the disk and run the Process. The model parameters not only include weights and biases, but also the LIF neuron parameters like threshold voltage. This flow is illustrated in the schematic below, along with the corresponding accuracies on a validation set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](ann_snn_conversion.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-09 15:17:43.886197: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nfs/slurm/intel-archi/lib\n",
      "2021-07-09 15:17:43.886261: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Assumes: lava root is in $PYTHONPATH\n",
    "import os\n",
    "import numpy as np\n",
    "from lava.core.generic.process import Process\n",
    "from lava.processes.generic.dense import Dense\n",
    "from lava.processes.generic.lif import LIF \n",
    "from lava.core.generic.enums import Backend\n",
    "\n",
    "# DatasetUtil class downloads MNIST data using Keras\n",
    "# and stores it in lava-mnist/datasets/MNIST directory\n",
    "from datasets.dataset_utils import DatasetUtil\n",
    "\n",
    "# DatasetBuffer class holds MNIST data\n",
    "from nets.mnist_execvar_io import DatasetBuffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### MNIST dataset\n",
    "We use a Keras convenience function to get the MNIST data and store it in\n",
    "`lava-mnist/datasets/MNIST` directory. Run the following cell if the dataset\n",
    "is locally unavailable in this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "Creating /home/dflorey/os3_lava/lava-mnist/datasets/MNIST\n"
     ]
    }
   ],
   "source": [
    "db = DatasetUtil()\n",
    "db.save_npz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNIST Process:\n",
    "\n",
    "Every hierarchical Process inherits from `Process` class. Every process needs to define its own `_build()` method, which contains the architectural specification of the proess.\n",
    "\n",
    "##### Note: \n",
    "As mentioned above, the `classify()` method exists as a separate method,\n",
    "because this tutorial deals with direct input injection and output readout\n",
    "using the `ExecVar` mechanism. This method performs the job of input\n",
    "injection by explicit register writes (the line that says `self.input_lif\n",
    ".bias_mant.value = new_biases`) as well as register readouts (the line that\n",
    "says `voltages = self.output_lif.v.value`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMnist(Process):\n",
    "    \"\"\"A simple MNIST digit classifying process, comprised by only LIF and\n",
    "    Dense processes. The architecture is: Input (784,) -> Dense (64,\n",
    "    ) -> Dense(64,) -> Dense(10,)\"\"\"\n",
    "    def _build(self, **kwargs):\n",
    "        dataset_path = kwargs.pop('dataset_path', '../datasets/MNIST')\n",
    "        trained_wgts = kwargs.pop('trained_weights_path',\n",
    "                                  './mnist_dense_only_snn_wblist.npy')\n",
    "        steps_per_image = kwargs.pop('num_steps_per_image', 512)\n",
    "\n",
    "        ###\n",
    "        # Code to read and load weights and biases; LIF parameters\n",
    "        # vth, bias_exp, dv, du are directly read from the SNN\n",
    "        # model generated by SNNToolBox.\n",
    "        ###\n",
    "        real_path_trained_wgts = os.path.realpath(trained_wgts)\n",
    "        wb_list = np.load(real_path_trained_wgts, allow_pickle=True)\n",
    "        w_dense1 = wb_list[0].transpose().astype(np.int32)\n",
    "        b_lif1 = wb_list[1].astype(np.int32)\n",
    "        w_dense2 = wb_list[2].transpose().astype(np.int32)\n",
    "        b_lif2 = wb_list[3].astype(np.int32)\n",
    "        w_dense3 = wb_list[4].transpose().astype(np.int32)\n",
    "        b_lif3 = wb_list[5].astype(np.int32)\n",
    "\n",
    "        ###\n",
    "        # Network definition\n",
    "        ###\n",
    "        self.data_buff = DatasetBuffer(dataset_path=dataset_path)\n",
    "        self.input_lif = LIF(size=784, bias_exp=6, vth=1528,\n",
    "                             dv=0, du=4095)\n",
    "        \n",
    "        self.dense1 = Dense(wgts=w_dense1)(s_in=self.input_lif.s_out)\n",
    "        self.lif1 = LIF(size=self.dense1.size[0], bias_mant=b_lif1,\n",
    "                        bias_exp=6, vth=380, dv=0, du=4095)(a_in=self.dense1.a_out)\n",
    "        self.dense2 = Dense(wgts=w_dense2)(s_in=self.lif1.s_out)\n",
    "        self.lif2 = LIF(size=self.dense2.size[0], bias_mant=b_lif2,\n",
    "                        bias_exp=6, vth=354, dv=0, du=4095)(a_in=self.dense2.a_out)\n",
    "        self.dense3 = Dense(wgts=w_dense3)(s_in=self.lif2.s_out)\n",
    "        \n",
    "        self.output_lif = LIF(size=self.dense3.size[0], bias_mant=b_lif3,\n",
    "                              bias_exp=6, vth=131071, dv=0, du=4095)(a_in=self.dense3.a_out)\n",
    "        self.predictions = []\n",
    "        self.ground_truths = []\n",
    "\n",
    "    def reset_process(self):\n",
    "        \"\"\"Reset the neuronal states bias, current, and voltage for \n",
    "        all LIF processes. This is done after every input, to flush \n",
    "        the old states.\"\"\"\n",
    "        self.input_lif.bias_mant.value = np.zeros((self.input_lif.size,),\n",
    "                                                  dtype=np.int32)\n",
    "        self.input_lif.u.value = np.zeros((self.input_lif.size,),\n",
    "                                          dtype=np.int32)\n",
    "        self.input_lif.v.value = np.zeros((self.input_lif.size,),\n",
    "                                          dtype=np.int32)\n",
    "        self.lif1.u.value = np.zeros((self.lif1.size,), dtype=np.int32)\n",
    "        self.lif1.v.value = np.zeros((self.lif1.size,), dtype=np.int32)\n",
    "        self.lif2.u.value = np.zeros((self.lif2.size,), dtype=np.int32)\n",
    "        self.lif2.v.value = np.zeros((self.lif2.size,), dtype=np.int32)\n",
    "        self.output_lif.u.value = np.zeros((self.output_lif.size,), dtype=np.int32)\n",
    "        self.output_lif.v.value = np.zeros((self.output_lif.size,), dtype=np.int32)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_new_biases(raw_x_data, img_id, size):\n",
    "        \"\"\"Convert raw input image data to integer bias values. Each \n",
    "        input value is scaled to [0, 255].\"\"\"\n",
    "        new_img_bias = np.int32((raw_x_data[img_id, :]) * 255)\n",
    "        new_img_bias = new_img_bias.reshape((size,))\n",
    "        return new_img_bias\n",
    "\n",
    "    def classify(self, backend=Backend.TF, n_img=10, n_st_img=512):\n",
    "        \"\"\"Loop over all n_img number of input images, with n_st_img \n",
    "        steps per input.\"\"\"\n",
    "        \n",
    "        print( '----------------------------------------------------', flush=True)\n",
    "        print(f'Running MNIST classifier with {backend.name} backend', flush=True)\n",
    "        print( '----------------------------------------------------', flush=True)\n",
    "\n",
    "        # Running for 0 time-steps just compiles the Process\n",
    "        self.run(num_steps=0, backend=backend)\n",
    "        \n",
    "        for img_id in range(n_img):\n",
    "            print(f'Classifying img id: {img_id}... ', end='', flush=True)\n",
    "            \n",
    "            # Compute biases for InputLIF neurons\n",
    "            new_biases = self.compute_new_biases(\n",
    "                raw_x_data=self.data_buff.x_test, img_id=img_id,\n",
    "                size=self.data_buff.out_size)\n",
    "            \n",
    "            # Write biases to InputLIF\n",
    "            self.input_lif.bias_mant.value = new_biases\n",
    "            \n",
    "            # Run the classifier for this input\n",
    "            self.run(num_steps=n_st_img, backend=backend)\n",
    "            \n",
    "            # Read out the voltage state of OutputLIF\n",
    "            voltages = self.output_lif.v.value\n",
    "            self.predictions.append(np.argmax(voltages))\n",
    "            self.ground_truths.append(np.argmax(self.data_buff.y_test[img_id]))\n",
    "            self.reset_process()\n",
    "            print(f' Done.\\tPredicted Label: {np.argmax(voltages)}\\tGround '\n",
    "                  f'Truth: {np.argmax(self.data_buff.y_test[img_id])}')\n",
    "\n",
    "        print(self.predictions, self.ground_truths)\n",
    "        accuracy = np.sum(np.array(self.predictions) == np.array(\n",
    "            self.ground_truths)) / len(self.ground_truths) * 100\n",
    "        print(f'{backend.name} execution was {accuracy} % accurate.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Run classification on Loihi and Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:  If weight tensor is sparse, use Sparse() process and pass the weight tensor as a tensorflow.SparseTensor.\n",
      "INFO:  If weight tensor is sparse, use Sparse() process and pass the weight tensor as a tensorflow.SparseTensor.\n",
      "INFO:  If weight tensor is sparse, use Sparse() process and pass the weight tensor as a tensorflow.SparseTensor.\n",
      "----------------------------------------------------\n",
      "Running MNIST classifier with TF backend\n",
      "----------------------------------------------------\n",
      "Classifying img id: 0... WARNING:tensorflow:From /home/dflorey/os3_lava/lava-tf-compiler/lava/core/tf/executor.py:270: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-09 15:17:54.769205: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /nfs/slurm/intel-archi/lib\n",
      "2021-07-09 15:17:54.769241: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-07-09 15:17:54.769265: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ncl-dev-01): /proc/driver/nvidia/version does not exist\n",
      "2021-07-09 15:17:54.769610: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Done.\tPredicted Label: 7\tGround Truth: 7\n",
      "Classifying img id: 1...  Done.\tPredicted Label: 2\tGround Truth: 2\n",
      "Classifying img id: 2...  Done.\tPredicted Label: 1\tGround Truth: 1\n",
      "Classifying img id: 3...  Done.\tPredicted Label: 0\tGround Truth: 0\n",
      "Classifying img id: 4...  Done.\tPredicted Label: 4\tGround Truth: 4\n",
      "Classifying img id: 5...  Done.\tPredicted Label: 1\tGround Truth: 1\n",
      "Classifying img id: 6...  Done.\tPredicted Label: 8\tGround Truth: 4\n",
      "Classifying img id: 7...  Done.\tPredicted Label: 9\tGround Truth: 9\n",
      "Classifying img id: 8...  Done.\tPredicted Label: 8\tGround Truth: 5\n",
      "Classifying img id: 9...  Done.\tPredicted Label: 9\tGround Truth: 9\n",
      "[7, 2, 1, 0, 4, 1, 8, 9, 8, 9] [7, 2, 1, 0, 4, 1, 4, 9, 5, 9]\n",
      "TF execution was 80.0 % accurate.\n"
     ]
    }
   ],
   "source": [
    "data_path = '../datasets/MNIST'\n",
    "wgts_path = '../trained_models/mnist_dense_only_snn_wblist.npy'\n",
    "num_steps_per_image = 512\n",
    "num_images = 10\n",
    "\n",
    "# Instantiate the classifier\n",
    "mnist_classifier = SimpleMnist(dataset_path=data_path,\n",
    "                               trained_weights_path=wgts_path,\n",
    "                               num_steps_per_image=num_steps_per_image)\n",
    "\n",
    "# Run on TF. Note how the backend is specified.\n",
    "mnist_classifier.classify(backend=Backend.TF, n_img=num_images,\n",
    "                          n_st_img=num_steps_per_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
